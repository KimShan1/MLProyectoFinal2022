---
title: "Untitled"
output: html_document
date: "2022-12-13"
---
```{r}
library(tidyverse)
library(tidymodels)
library(recipes)
library(plyr)
library(dplyr)
library(rsample)
library(feather)
library(arrow)
library(patchwork)
library(yardstick)
library(parsnip)
library(xgboost)
library(purrr)
library(caret)
library(tidyr)
library(lubridate)
library(ggforce)
```
# Trabajo final para la materia Aprendizaje de Máquina

Este trabajo busca crear un modelo para predecir la probabilidad de incumplimiento de un pago por parte de un cliente de American Express. 

La base de datos original incluye 191 variables anonimizadas y normalizadas, divididas en datos de delinquency (D_\*), gasto (S_\*), pago (P_\*), balance (B_\*) y riesgo (R_\*). La base de datos incluye en total más de 450 mil clientes.

La base de datos fue publicada en la siguiente liga: https://www.kaggle.com/c/amex-default-prediction

De inicio, no se pudo abrir directamente el csv porque pesaba más de 16GB, por lo que se recurrió a convertir la base a archivo tipo feather, con un resultado de casi 7GB de los más de 16 GB que pesaba la base original. Dado que la RAM de la computadora es de 8 GB, se pudo leer después de cierto tiempo que tardó en compilarse.

```{python}
import numpy as np
import pandas as pd
#amex=pd.read_csv("~/Downloads/amex-default-prediction/train_data.csv")
#amex_fe=amex.to_feather("amex-data.feather")
target=pd.read_csv("~/Downloads/amex-default-prediction/train_labels.csv")
labels_fe=target.to_feather("amex-label.feather")
amex1<-amex %>%full_join(target)
split1<-initial_split(amex1,prop=0.85)
entrena0<-training(split1)
prueba0<-testing(split1)
```

## XGBOOST con Parsnip

Este modelo no se pudo usar porque no trataba NAs, con una base preprocesada sin NA tenía un recall de 0.922 y un accuracy de 0.87

```{r}
#planteamiento de receta
receta1<- recipe(target ~.,data=entrena0) %>% step_dummy(all_nominal_predictors())

#modelo y workflow
boosting1<-boost_tree(learn_rate = tune(), trees = tune(), mtry = tune(), tree_depth =6) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")
flujo1<-workflow() %>% add_recipe(receta1) %>% add_model(boosting1)

#gridsearch 
r_split <- manual_rset(list(split1), "rset1")
metricas1<-metric_set(yardstick::precision,yardstick::recall,sens,spec)
valores_grid <- expand_grid(learn_rate=10**seq(-8,0,0.5),mtry=seq(1,10,2), trees=seq(1,10,2))
grid11<-tune_grid(flujo1,resamples=r_split,grid = valores_grid,metrics = metricas1)
```



Resalta que tenemos desbalance en los datos, que se traduce en una baja proporción de individuos morosos; además, es importante no clasificar morosos como no morosos. 

Por ello, es muy importante minimizar falsos negativos, por lo que el algoritmo se concentra en maximizar la sensibilidad (1-tasa de falsos negativos). Se muestra la matriz de confusión para el conjunto de prueba.

```{r}
#mejor grid
metricas11<-collect_metrics(grid11)
mejor<-select_best(grid11,metric='sens')
ajuste_1 <- finalize_workflow(flujo1, mejor) |>  fit(entrena0)
predict1<-predict(ajuste_1,prueba0)

#matriz de confusión
matriz_prueba<-data.frame(prediccion=predict1$.pred_class,verdad=prueba0$default)%>%mutate(prediccion=prediccion,verdad=verdad)
matriz_prueba %>% confusionMatrix(prediccion,verdad) 
```


