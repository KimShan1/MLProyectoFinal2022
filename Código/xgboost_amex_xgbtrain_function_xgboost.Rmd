---
title: "Trabajo final para aprendizaje de máquina"
output: html_document
date: "2022-12-01"
---
```{r}
library(tidyverse)
library(tidymodels)
library(recipes)
library(plyr)
library(dplyr)
library(rsample)
library(feather)
library(arrow)
library(patchwork)
library(yardstick)
library(parsnip)
library(xgboost)
library(purrr)
library(caret)
library(tidyr)
library(lubridate)
library(ggforce)
```
# Trabajo final para la materia Aprendizaje de Máquina

Este trabajo busca crear un modelo para predecir la probabilidad de incumplimiento de un pago por parte de un cliente de American Express. 


# Base de datos agregada
```{r}
amex_ultimo<-read_csv("~/Downloads/train_ultimo_per.csv")
amex_target<-read_csv("~/Downloads/amex-default-prediction/train_labels.csv")
amex_ultimo0<-amex_ultimo %>% left_join(amex_target) %>% select(-customer_ID,-D_87) %>% mutate(B_30=as.
                                                                                               tor(B_30), B_38=as.factor(B_38), D_114=as.factor(D_114), D_117=as.factor(D_117), D_120=as.factor(D_120), D_126=as.factor(D_126),  D_68=as.factor(D_68))

dv<-dummyVars("target~.",amex_ultimo0)
onehot<-data.frame(predict(dv,amex_ultimo0)) %>% cbind(target=amex_target$target)
```

Dividimos datos en entrenamiento y validación. Dado que tenemos un considerable número de observaciones, el set de entrenamiento puede tener una proporción considerable sin perjuicio al set de prueba. Cabe señalar que para la funciónxgboost::xgb.train se pedía una matrix con formato xgb.DMatrix, lo cual a su vez requería separar la variable objetivo de las explicativas una vez realizada la división de sets de entrenamiento y validación.

```{r}
set.seed(10000)
split1<-initial_split(onehot,prop=0.85)
entrena0<-training(split1) %>%mutate(target=as.factor(target))
prueba0<-testing(split1) %>%mutate(target=as.factor(target))

```


# Planteamiento del modelo XGBOOST

Debido a que se trata de un problema de clasificación, uno de los modelos que puede utilizarse para predecir la variable objetivo es XGBOOST, consiste en una mejora a los modelos de decisión de árboles binomiales, con la adición de que los residuos entre la última predicción y el valor real se convierten en el objetivo de la predicción siguiente.

## XGboost usando libreria XGBOOST
se prefiere la librería caret porque se encontró que mide performance con accuracy, muy cercano a sensibilidad

```{r}
#creación de xgb.DMatrix
ent0x<-entrena0%>%select(-target)
ent0y<-entrena0%>%select(target)
dent0<-xgb.DMatrix(data=as.matrix(ent0x),label=as.matrix(ent0y))
prue0x<-prueba0%>%select(-target)
prue0y<-prueba0%>%select(target)
dprue0<-xgb.DMatrix(data=as.matrix(prue0x),label=as.matrix(prue0y))

watchlist1<-list(train=dent0,test=dprue0)

param1<-list(max.depth = 4, verbose=0,
               eta = 1, nthread = 2, nrounds = 10, lambda=0.5, alpha=0.5)
#bst1 <- xgboost(param1,entrena0,nrounds=2,missing=NA)
boost1<-xgb.train(data=dent0,
                params=param1,nrounds=2,verbose=0,maximize=T)
bst = xgb.train(data = dent0, 
                max.depth = 8, 
                eta = 0.3, 
                nthread = 2, 
                nround = 10, 
                eval_metric="auc",
                watchlist = watchlist1, 
                objective = "binary:logistic", 
                early_stopping_rounds = 50,
                missing=NA,
                verbose=0)
```

